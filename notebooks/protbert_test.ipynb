{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import MultilabelAccuracy\n",
    "from torchmetrics.classification import MultilabelF1Score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import random_split\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# TORCH MODULES FOR METRICS COMPUTATION :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/gscratch/rao/aresf/Code/CAFA5/data\"\n",
    "\n",
    "\n",
    "class config:\n",
    "    train_sequences_path = data_dir + \"/Train/train_sequences.fasta\"\n",
    "    train_labels_path = data_dir + \"/Train/train_terms.tsv\"\n",
    "    test_sequences_path = data_dir + \"/Test/testsuperset.fasta\"\n",
    "\n",
    "    num_labels = 500\n",
    "    n_epochs = 5\n",
    "    batch_size = 128\n",
    "    lr = 0.001\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(config):\n",
    "    print(\n",
    "        \"GENERATE TARGETS FOR ENTRY IDS (\"\n",
    "        + str(config.num_labels)\n",
    "        + \" MOST COMMON GO TERMS)\"\n",
    "    )\n",
    "    ids = np.load(\"../data/train_ids.npy\")\n",
    "    labels = pd.read_csv(config.train_labels_path, sep=\"\\t\")\n",
    "\n",
    "    top_terms = labels.groupby(\n",
    "        \"term\")[\"EntryID\"].count().sort_values(ascending=False)\n",
    "    labels_names = top_terms[: config.num_labels].index.values\n",
    "    train_labels_sub = labels[\n",
    "        (labels.term.isin(labels_names)) & (labels.EntryID.isin(ids))\n",
    "    ]\n",
    "    id_labels = train_labels_sub.groupby(\n",
    "        \"EntryID\")[\"term\"].apply(list).to_dict()\n",
    "\n",
    "    go_terms_map = {label: i for i, label in enumerate(labels_names)}\n",
    "    labels_matrix = np.empty((len(ids), len(labels_names)))\n",
    "\n",
    "    for index, id in tqdm(enumerate(ids)):\n",
    "        id_gos_list = id_labels[id]\n",
    "        temp = [go_terms_map[go] for go in labels_names if go in id_gos_list]\n",
    "        labels_matrix[index, temp] = 1\n",
    "\n",
    "    labels_list = []\n",
    "    for l in range(labels_matrix.shape[0]):\n",
    "        labels_list.append(labels_matrix[l, :])\n",
    "\n",
    "    labels_df = pd.DataFrame(data={\"EntryID\": ids, \"labels_vect\": labels_list})\n",
    "    labels_df.to_pickle(\"../data/train_targets_top\" +\n",
    "                        str(config.num_labels) + \".pkl\")\n",
    "    print(\"GENERATION FINISHED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for the different embedding vectors :\n",
    "embeds_map = {\n",
    "    \"T5\": \"t5embeds\",\n",
    "    \"ProtBERT\": \"protbert-embeddings-for-cafa5\",\n",
    "    \"EMS2\": \"cafa-5-ems-2-embeddings-numpy\",\n",
    "}\n",
    "\n",
    "# Length of the different embedding vectors :\n",
    "embeds_dim = {\"T5\": 1024, \"ProtBERT\": 1024, \"EMS2\": 1280}\n",
    "\n",
    "\n",
    "class ProteinSequenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, datatype, embeddings_source):\n",
    "        super(ProteinSequenceDataset).__init__()\n",
    "        self.datatype = datatype\n",
    "\n",
    "        if embeddings_source in [\"ProtBERT\", \"EMS2\"]:\n",
    "            embeds = np.load(\"../data\" + \"/\" + datatype + \"_embeddings.npy\")\n",
    "            ids = np.load(\"../data\" + \"/\" + datatype + \"_ids.npy\")\n",
    "\n",
    "        embeds_list = []\n",
    "        for l in range(embeds.shape[0]):\n",
    "            embeds_list.append(embeds[l, :])\n",
    "        self.df = pd.DataFrame(data={\"EntryID\": ids, \"embed\": embeds_list})\n",
    "\n",
    "        if datatype == \"train\":\n",
    "            df_labels = pd.read_pickle(\n",
    "                \"../data/train_targets_top\" + str(config.num_labels) + \".pkl\"\n",
    "            )\n",
    "            self.df = self.df.merge(df_labels, on=\"EntryID\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        embed = torch.tensor(self.df.iloc[index][\"embed\"], dtype=torch.float32)\n",
    "        if self.datatype == \"train\":\n",
    "            targets = torch.tensor(\n",
    "                self.df.iloc[index][\"labels_vect\"], dtype=torch.float32\n",
    "            )\n",
    "            return embed, targets\n",
    "        if self.datatype == \"test\":\n",
    "            id = self.df.iloc[index][\"EntryID\"]\n",
    "            return embed, id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(input_dim, 1012)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(1012, 712)\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(712, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(CNN1D, self).__init__()\n",
    "        # (batch_size, channels, embed_size)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=3,\n",
    "                               kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        # (batch_size, 3, embed_size)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # (batch_size, 3, embed_size/2 = 512)\n",
    "        self.conv2 = nn.Conv1d(in_channels=3, out_channels=8,\n",
    "                               kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        # (batch_size, 8, embed_size/2 = 512)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # (batch_size, 8, embed_size/4 = 256)\n",
    "        self.fc1 = nn.Linear(in_features=int(\n",
    "            8 * input_dim/4), out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "        x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(embeddings_source, model_type=\"linear\", train_size=0.9):\n",
    "\n",
    "    train_dataset = ProteinSequenceDataset(\n",
    "        datatype=\"train\", embeddings_source=embeddings_source)\n",
    "\n",
    "    train_set, val_set = random_split(train_dataset, lengths=[int(len(\n",
    "        train_dataset)*train_size), len(train_dataset)-int(len(train_dataset)*train_size)])\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=config.batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_set, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    if model_type == \"linear\":\n",
    "        model = MultiLayerPerceptron(\n",
    "            input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels).to(config.device)\n",
    "    if model_type == \"convolutional\":\n",
    "        model = CNN1D(input_dim=embeds_dim[embeddings_source],\n",
    "                      num_classes=config.num_labels).to(config.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=1)\n",
    "    CrossEntropy = torch.nn.CrossEntropyLoss()\n",
    "    f1_score = MultilabelF1Score(\n",
    "        num_labels=config.num_labels).to(config.device)\n",
    "    n_epochs = config.n_epochs\n",
    "\n",
    "    print(\"BEGIN TRAINING...\")\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    train_f1score_history = []\n",
    "    val_f1score_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"EPOCH \", epoch+1)\n",
    "        # TRAIN PHASE :\n",
    "        losses = []\n",
    "        scores = []\n",
    "        for embed, targets in tqdm(train_dataloader):\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(embed)\n",
    "            loss = CrossEntropy(preds, targets)\n",
    "            score = f1_score(preds, targets)\n",
    "            losses.append(loss.item())\n",
    "            scores.append(score.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_score = np.mean(scores)\n",
    "        print(\"Running Average TRAIN Loss : \", avg_loss)\n",
    "        print(\"Running Average TRAIN F1-Score : \", avg_score)\n",
    "        train_loss_history.append(avg_loss)\n",
    "        train_f1score_history.append(avg_score)\n",
    "\n",
    "        # VALIDATION PHASE :\n",
    "        losses = []\n",
    "        scores = []\n",
    "        for embed, targets in val_dataloader:\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device)\n",
    "            preds = model(embed)\n",
    "            loss = CrossEntropy(preds, targets)\n",
    "            score = f1_score(preds, targets)\n",
    "            losses.append(loss.item())\n",
    "            scores.append(score.item())\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_score = np.mean(scores)\n",
    "        print(\"Running Average VAL Loss : \", avg_loss)\n",
    "        print(\"Running Average VAL F1-Score : \", avg_score)\n",
    "        val_loss_history.append(avg_loss)\n",
    "        val_f1score_history.append(avg_score)\n",
    "\n",
    "        scheduler.step(avg_loss)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(\"TRAINING FINISHED\")\n",
    "    print(\"FINAL TRAINING SCORE : \", train_f1score_history[-1])\n",
    "    print(\"FINAL VALIDATION SCORE : \", val_f1score_history[-1])\n",
    "\n",
    "    losses_history = {\"train\": train_loss_history, \"val\": val_loss_history}\n",
    "    scores_history = {\"train\": train_f1score_history,\n",
    "                      \"val\": val_f1score_history}\n",
    "\n",
    "    return model, losses_history, scores_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN TRAINING...\n",
      "EPOCH  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:17<00:00, 55.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  141.17737231030688\n",
      "Running Average TRAIN F1-Score :  0.05762168635185305\n",
      "Running Average VAL Loss :  139.7490392412458\n",
      "Running Average VAL F1-Score :  0.08033394248091749\n",
      "\n",
      "\n",
      "EPOCH  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:15<00:00, 66.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  138.0984937075254\n",
      "Running Average TRAIN F1-Score :  0.10020145817429989\n",
      "Running Average VAL Loss :  137.7405114855085\n",
      "Running Average VAL F1-Score :  0.11651615951476353\n",
      "\n",
      "\n",
      "EPOCH  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:14<00:00, 68.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  136.762160645141\n",
      "Running Average TRAIN F1-Score :  0.12057363572386237\n",
      "Running Average VAL Loss :  136.51682104383195\n",
      "Running Average VAL F1-Score :  0.12397308521238821\n",
      "\n",
      "\n",
      "EPOCH  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:14<00:00, 66.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  136.02096755021103\n",
      "Running Average TRAIN F1-Score :  0.13082024394036768\n",
      "Running Average VAL Loss :  136.07698331560408\n",
      "Running Average VAL F1-Score :  0.13541990106127091\n",
      "\n",
      "\n",
      "EPOCH  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:15<00:00, 65.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  135.45669509933427\n",
      "Running Average TRAIN F1-Score :  0.13802178239042348\n",
      "Running Average VAL Loss :  135.967351096017\n",
      "Running Average VAL F1-Score :  0.14094462125961268\n",
      "\n",
      "\n",
      "TRAINING FINISHED\n",
      "FINAL TRAINING SCORE :  0.13802178239042348\n",
      "FINAL VALIDATION SCORE :  0.14094462125961268\n"
     ]
    }
   ],
   "source": [
    "model, losses, scores = train_model(\n",
    "    embeddings_source=\"ProtBERT\", model_type=\"convolutional\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
